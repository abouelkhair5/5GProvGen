{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# Some of the code is adapted from:\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "##########################################################################################\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import JODIEDataset\n",
    "from torch_geometric.datasets import ICEWS18\n",
    "# from torch_geometric.loader import TemporalDataLoader\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, LastAggregator)\n",
    "from torch_geometric import *\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg structure:      [src_node_feature,edge_attr,dst_node_feature]\n",
    "data_dir = \"../data/camflow250\"\n",
    "train_data=torch.load(f\"{data_dir}/graph_1.TemporalData\")\n",
    "train_data1=torch.load(f\"{data_dir}/graph_2.TemporalData\")\n",
    "train_data2=torch.load(f\"{data_dir}/graph_3.TemporalData\")\n",
    "train_data3=torch.load(f\"{data_dir}/graph_4.TemporalData\")\n",
    "train_data4=torch.load(f\"{data_dir}/graph_5.TemporalData\")\n",
    "train_data5=torch.load(f\"{data_dir}/graph_6.TemporalData\")\n",
    "train_data6=torch.load(f\"{data_dir}/graph_7.TemporalData\")\n",
    "train_data7=torch.load(f\"{data_dir}/graph_8.TemporalData\")\n",
    "train_data8=torch.load(f\"{data_dir}/graph_9.TemporalData\")\n",
    "train_data9=torch.load(f\"{data_dir}/graph_10.TemporalData\")\n",
    "# test_data=torch.load(\"../data/graph_305.TemporalData\")\n",
    "val_data=torch.load(f\"{data_dir}/graph_40.TemporalData\")\n",
    "\n",
    "def graph_merge(a,b):\n",
    "    merged=a\n",
    "    merged.msg=torch.cat([merged.msg,b.msg],dim=0)\n",
    "    merged.src=torch.cat([merged.src,b.src],dim=0)\n",
    "    merged.dst=torch.cat([merged.dst,b.dst],dim=0)\n",
    "    merged.t=torch.cat([merged.t,b.t],dim=0)\n",
    "    return merged\n",
    "\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data1))\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data2))\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data3))\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data4))\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data5))\n",
    "train_data=graph_merge((train_data),copy.deepcopy(train_data6))\n",
    "# train_data=graph_merge((train_data),copy.deepcopy(train_data7))\n",
    "# train_data=graph_merge((train_data),copy.deepcopy(train_data8))\n",
    "# train_data=graph_merge((train_data),copy.deepcopy(train_data9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node_num = 10090000\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "neighbor_loader = LastNeighborLoader(max_node_num, size=5, device=device)\n",
    "\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels // 2, heads=2,\n",
    "                                    dropout=0.1, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        return self.conv(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels)\n",
    "        self.lin_dst = Linear(in_channels, in_channels)\n",
    "        self.lin_final = Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = self.lin_src(z_src) + self.lin_dst(z_dst)\n",
    "        h = h.relu()\n",
    "        return self.lin_final(h)\n",
    "\n",
    "\n",
    "memory_dim = time_dim = embedding_dim = 200\n",
    "\n",
    "memory = TGNMemory(\n",
    "    max_node_num,\n",
    "    train_data.msg.size(-1),\n",
    "    memory_dim,\n",
    "    time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters())\n",
    "    | set(link_pred.parameters()), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "BATCH=1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    # train_loader = TemporalDataLoader(train_data, batch_size=BATCH)\n",
    "\n",
    "    for batch in train_data.seq_batches(batch_size=BATCH):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "\n",
    "        # Sample negative destination nodes.\n",
    "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0),),\n",
    "                                dtype=torch.long, device=device)\n",
    "        #         edge_i = torch.vstack([src, pos_dst])\n",
    "        #         neg_src, neg_dst = negative_sampling(edge_i)\n",
    "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
    "        #         n_id = torch.cat([src, pos_dst, neg_src, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]])\n",
    "\n",
    "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "        #         print(z.shape)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "    #     print(\"trained_stage_data:\",train_data)\n",
    "    return total_loss / train_data.num_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test_new(inference_data):\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "    torch.manual_seed(12345)  # Ensure determi|nistic sampling across epochs.\n",
    "\n",
    "    aps, aucs = [], []\n",
    "    pos_o = []\n",
    "\n",
    "    # test_loader = TemporalDataLoader(inference_data, batch_size=BATCH)\n",
    "    for batch in inference_data.seq_batches(batch_size=BATCH):\n",
    "        batch = batch.to(device)\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        neg_dst = torch.randint(min_dst_idx, max_dst_idx + 1, (src.size(0),),\n",
    "                                dtype=torch.long, device=device)\n",
    "\n",
    "        n_id = torch.cat([src, pos_dst, neg_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "        neg_out = link_pred(z[assoc[src]], z[assoc[neg_dst]])\n",
    "        pos_o.append(pos_out)\n",
    "\n",
    "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "\n",
    "    loss = total_loss / inference_data.num_events\n",
    "    return float(torch.tensor(aps).mean()), float(\n",
    "        torch.tensor(aucs).mean()), pos_out.sigmoid().to(device), neg_out.sigmoid().to(device), loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 01, Loss: 0.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [07:32<1:07:48, 452.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.4596\n",
      "  Epoch: 02, Loss: 0.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [14:12<56:12, 421.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3888\n",
      "  Epoch: 03, Loss: 0.2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [20:56<48:15, 413.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3719\n",
      "  Epoch: 04, Loss: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [27:42<41:03, 410.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3674\n",
      "  Epoch: 05, Loss: 0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [34:16<33:43, 404.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3610\n",
      "  Epoch: 06, Loss: 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [40:49<26:42, 400.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3541\n",
      "  Epoch: 07, Loss: 0.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [47:15<19:47, 395.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3466\n",
      "  Epoch: 08, Loss: 0.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [53:43<13:06, 393.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3468\n",
      "  Epoch: 09, Loss: 0.2542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:00:16<06:33, 393.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3411\n",
      "  Epoch: 10, Loss: 0.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:07:03<00:00, 402.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_data: Loss: 1.3216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, 11)):\n",
    "    loss = train()\n",
    "    print(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "\n",
    "    test_ap, test_auc, pos_out_test, neg_out_test, loss_test = test_new(val_data)\n",
    "    print(f'val_data: Loss: {loss_test:.4f}')\n",
    "\n",
    "model = [memory, gnn, link_pred, neighbor_loader]\n",
    "torch.save(model, f\"{data_dir}/model_saved.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kairos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
